from keras.preprocessing.sequence import pad_sequences
from gensim.models import Word2Vec
from gensim.corpora.dictionary import Dictionary
from random import choice
import numpy as np
import random
import MeCab
import math
import json
import gc
import os
import re


def data_input(input_dir):
    """
    load document from files and make sentence segmentation
    Args:
        input_dir (path): relative path to the data folder
    Returns:
        all_texts(list): matrix generated after document sentence segmentation
    """
    file_list = []
    if not os.path.exists(input_dir):
        os.makedirs(input_dir)
    for f in os.listdir(input_dir):
        file_list += [input_dir + '\\' + f]

    all_texts = []
    for fi in file_list:
        the_text = []
        with open(fi, encoding='utf8') as file_input:
            the_document = ("".join(file_input.readlines())).replace("\n", "")
            the_document = the_document.replace("\n\r", "")
            the_text += the_document.split('。')
        all_texts += [the_text[:-1]]
    return all_texts


def data_cleaning(the_document):
    """
    Remove punctuation marks and special characters, and then do word segmentation
    Args:
        the_document (list): document after sentence segmentation
    Returns:
        the_document_3(list): pure document after word segmentation
    """
    mecab = MeCab.Tagger("-Owakati")
    the_document_1 = re.sub(u'[^\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FA5\uF900-\uFA2D]', "", "".join(the_document))
    the_document_2 = mecab.parse(the_document_1).strip()
    the_document_3 = the_document_2.split(' ')
    return the_document_3


def data_plagiarism(the_document):
    """
    Simulate article plagiarism, remove some sentences and switch some sentences
    Args:
        the_document (list): original document after sentence segmentation
    Returns:
        the_document_3(list): pure document after word segmentation
    """
    some = random.sample(the_document, math.ceil(0.002 * len(the_document)))
    for x in some:
        the_document.remove(x)

    the = random.sample(range(0, len(the_document)), math.ceil(0.005 * len(the_document)))
    plag = list(the)
    plag_document = list(the_document)
    random.shuffle(the)
    for x, y in zip(plag, the):
        plag_document[x] = the_document[y]

    document_plagiarism = data_cleaning(plag_document)
    return document_plagiarism


def data_load(data_dir):
    """
    Load data from file system, remove special characters, do word segmentation and produce control group
    Args:
        data_dir (path): relative path to the data folder
    Returns:
        documents1(list): list of word list of original document
        documents2(list): list of word list of control group document
        is_similar(list): list containing labels if respective documents in document1 and document2 are same or not
                         (1 if same else 0)
    """
    documents1 = []
    documents2 = []
    is_similar = []
    input_dir = os.path.abspath(os.path.join(data_dir, "input"))
    all_texts = data_input(input_dir)

    for document in all_texts:
        document1 = data_cleaning(document)
        similar_value = np.random.randint(0, 2)
        if similar_value == 0:
            other_texts = all_texts.copy()
            other_texts.remove(document)
            document2 = data_cleaning(choice(other_texts))
        elif similar_value == 1:
            document2 = data_plagiarism(document)
        else:
            print("###!!Warning: wrong similar_value.###")
            break
        documents1 += [document1]
        documents2 += [document2]
        is_similar += str(similar_value)
#        if len(document1)> 400:
#            num +=1
#    print("%d-%d"%(len(is_similar),num))
    return documents1, documents2, is_similar


def data_predict(data_dir):
    """
    Load data from file system, remove special characters, do word segmentation and produce control group
    Args:
        data_dir (path): relative path to the data folder
    Returns:
        documents1(list): list of word list of original document
        documents2(list): list of word list of control group document
    """
    input_dir = os.path.abspath(os.path.join(data_dir, "predict"))
    all_texts = data_input(input_dir)

    for document in all_texts:
        data_cleaning(document)

    documents1 = list(all_texts[::2])
    documents2 = list(all_texts[1::2])

    return documents1, documents2


def parse_dataset(w2indx, the_documents):  
    """
    Map words into index
    Args:
        w2indx(dict): dict containing words and their respective index
        the_documents(list): list of word list of document
    Returns:
        document_index(array):array of input features for train set or test set, which is built by word index 
    """
    data = []
    for document in the_documents:
        new_txt = []
        for word in document:
            if word in w2indx:
                new_txt.append(w2indx[word])
            else:
                new_txt.append(0)
        data.append(new_txt)
    document_index = np.array(data)
    return document_index


def create_dictionaries(model=None, combine=None):
    """
    Create dictionary through word2vector, and translate the word of documents into its index
    Args:
        model(model): trained word2vec model
        combine(list): list of documents after word segmentation
    Returns:
        w2indx(dict): dict containing words and their respective index
        w2vec(dict): dict containing words and their respective vectors
        combined(array):array of input features for train set or test set, which is built by word index
    """
    if (combine is not None) and (model is not None):
        gensim_dict = Dictionary()
        gensim_dict.doc2bow(model.wv.vocab.keys(),allow_update=True)  #把该查询文档（词集）更改为（词袋模型）即：字典格式，key是单词，value是该单词在该文档中出现次数。
        w2indx = {v: k+1 for k, v in gensim_dict.items()}  #所有的词语的索引
        w2vec = {word: model[word] for word in w2indx.keys()}  #所有词语的词向量
        combined = parse_dataset(w2indx=w2indx, the_documents=combine)  #每个句子所含词语对应的词语索引
        return w2indx, w2vec, combined
    else:
        print('No data provided...')


def train_word2vec(combine, vocab_dim, min_count, window_size, n_iterations, data_dir):
    """
    Train word2vector over training documents
    Args:
        combine (list): list of documents after word segmentation
        others: config of word2vec model
    Returns:
        n_symbols(int): size of vocabulary
        embedding_weights(dict): dict with word_index and vector mapping
        combined(list):list of input features for train set or test set, which is built by word index
    """
    model_dir = os.path.abspath(os.path.join(data_dir, "model"))
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)

    model = Word2Vec(size=vocab_dim, min_count=min_count, window=window_size, iter=n_iterations, sorted_vocab=True)
    model.build_vocab(combine)  #建立词典，必须步骤，不然会报错
    model.train(combine, total_examples=model.corpus_count, epochs=model.iter)  #训练词向量模型
    model.save(os.path.join(model_dir,  "Word2vec_model.pkl"))  #保存词向量模型
    index_dict, word_vectors, combined = create_dictionaries(model=model, combine=combine)
    n_symbols = len(index_dict) + 1  #所有单词的索引数，频数小于10的词语索引为0，所以加1
    embedding_weights = np.zeros((n_symbols, vocab_dim))  #索引为0的词语，词向量全为0
    for word, index in index_dict.items():  #从索引为1的词语开始，对每个词语对应其词向量
        embedding_weights[index, :] = word_vectors[word]

    f = open(os.path.join(model_dir, "w2index.txt"), 'w')   #save index_dict
    w2index = json.dumps(index_dict)
    f.write(str(w2index))
    f.close()
    return n_symbols, embedding_weights, combined


def create_train_dev_set(documents_pair, is_similar, max_len, validation_split_ratio):
    """
    Create training and validation dataset
    Args:
        documents_pair (list): list of tuple of sentences pairs
        is_similar (list): list containing labels if respective sentences in sentence1 and sentence2
                           are same or not (1 if same else 0)
        max_len (int): max sequence length of sentences to apply padding
        validation_split_ratio (float): contain ratio to split training data into validation data

    Returns:
        train_data_1 (list): list of input features for training set from sentences1
        train_data_2 (list): list of input features for training set from sentences2
        labels_train (np.array): array containing similarity score for training data
        leaks_train(np.array): array of training leaks features

        val_data_1 (list): list of input features for validation set from sentences1
        val_data_2 (list): list of input features for validation set from sentences1
        labels_val (np.array): array containing similarity score for validation data
        leaks_val (np.array): array of validation leaks features
    """
    documents1 = [x[0] for x in documents_pair]
    documents2 = [x[1] for x in documents_pair]

    leaks = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]
             for x1, x2 in zip(documents1, documents2)]

    # maxlen设置最大的序列长度，长于该长度的序列将会截短，短于该长度的序列将会填充
    train_padded_data_1 = pad_sequences(documents1, maxlen=max_len)
    train_padded_data_2 = pad_sequences(documents1, maxlen=max_len)
    train_labels = np.array(is_similar)
    leaks = np.array(leaks)

    shuffle_indices = np.random.permutation(np.arange(len(train_labels)))   #打乱顺序
    train_data_1_shuffled = train_padded_data_1[shuffle_indices]
    train_data_2_shuffled = train_padded_data_2[shuffle_indices]
    train_labels_shuffled = train_labels[shuffle_indices]
    leaks_shuffled = leaks[shuffle_indices]

    dev_idx = max(1, int(len(train_labels_shuffled) * validation_split_ratio))

    del train_padded_data_1  #释放内存
    del train_padded_data_2
    gc.collect()

    train_data_1, val_data_1 = train_data_1_shuffled[:-dev_idx], train_data_1_shuffled[-dev_idx:]   #划分训练集验证集
    train_data_2, val_data_2 = train_data_2_shuffled[:-dev_idx], train_data_2_shuffled[-dev_idx:]
    labels_train, labels_val = train_labels_shuffled[:-dev_idx], train_labels_shuffled[-dev_idx:]
    leaks_train, leaks_val = leaks_shuffled[:-dev_idx], leaks_shuffled[-dev_idx:]

    return train_data_1, train_data_2, labels_train, leaks_train, val_data_1, val_data_2, labels_val, leaks_val


def create_test_data(index_dict, test_documents_pair, max_document_length):
    """
    Create training and validation dataset
    Args:
        index_dict(dict): dict containing words and their respective index
        test_documents_pair (list): list of tuple of documents pairs
        max_document_length (int): max document length of documents to apply padding

    Returns:
        test_data_1 (list): list of input features for training set from test_documents1
        test_data_2 (list): list of input features for training set from test_documents2
    """
    test_documents1 = [data_cleaning(x[0]) for x in test_documents_pair]
    test_documents2 = [data_cleaning(x[1]) for x in test_documents_pair]
    test_documents_1 = parse_dataset(index_dict, test_documents1)
    test_documents_2 = parse_dataset(index_dict, test_documents2)


    leaks_test = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]
                  for x1, x2 in zip(test_documents_1, test_documents_2)]

    leaks_test = np.array(leaks_test)
    test_data_1 = pad_sequences(test_documents_1, maxlen=max_document_length)
    test_data_2 = pad_sequences(test_documents_2, maxlen=max_document_length)

    return test_data_1, test_data_2, leaks_test
